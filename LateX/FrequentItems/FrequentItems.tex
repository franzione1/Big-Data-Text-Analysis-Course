\section{Discovering frequent itemsets}
Ora ci occupiamo della ricerca dei \textit{frequent itemsets}. Immaginando il caso di un supermercato, andiamo a cercare gli elementi che la gente acquista insieme. 
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.60]{FrequentItems/img/frq_itm.png}
\end{figure}
\\
E dopo aver trovato queste coppie frequenti, posso estrarre delle regole da essi:
\begin{itemize}
    \item Se qualcuno compra diaper e milk, dovrebbe comprare anche beer
    \item Discorso analogo per bread
\end{itemize}
Ma attenzione a non pensare a regole biunivoche! Sono definite in un solo verso. 

\subsection{Market-Basket model}
Il modello market-basket è utilizzato per descrivere una relazione di tipo many-many tra due tipi di oggetti. Tutti i basket hanno al loro interno degli items. Un insieme di elementi che appare \textbf{frequentemente} è un set che si ripete più volte all'interno dell'insieme dei basket. Ovvero, ci saranno n basket dove questa coppia sarà presente. Quandol è abbastanza per essere frequente? Quando n $>$ s, dove \textit{s} è un parametro chiamato \textit{soglia di supporto.}
\\
Questo parametro è importante perchè deteremina la capacità di estrapolare delle regole da un dataset. 
\begin{itemize}
    \item Se s è molto basso, ovvero ad esempio la soglia minima è che s = 1, allora riuscirò a scrivere tantissime regole, ma risulteranno completamente inutili
    \item Se s è molto alto, avviene esattamente il contrario, potrei descrivere troppo poco il dataset e impare poco sulle relazioni tra i dati
\end{itemize}
Un esempio di valore per s corrisponde all'1\% dei valori. \textbf{Attenzione!} Soglia di supporto e supporto sono diversi. Il supporto è il numero di volte che una coppia appare nel dataset, mentre la soglia è impostata per decidere se una coppia è frequente. 
\subsubsection{Principio di monotonicità}
Regola che limita il numero di frequent itemsets:
\begin{center}
    \textit{In un dataset qualunque, se sono presenti due elementi x e y frequent, avranno sicuramente una frequenza maggiore o uguale alla coppia (x,y)}
\end{center}
Ovviamente, è uguale se i due valori si ripetono sempre in tutti i basket di cui fanno parte, maggiore nel caso in cui la coppia non sia costante.

\newpage

\subsubsection{Confidence}
Un modo per misurare quanto una regola sia affidabile rispetto ad un'altra è il concetto di \textit{confidenza}.
\begin{center}
        \begin{math}
            conf(I \rightarrow j) = \frac{support(I  \cup  j)}{support(I)}
        \end{math}
\end{center}
Definita anche come la probabilità di j dato I. La utilizzo per trovare le regole che meglio descrivono i miei dati. Infatti, prendendo solo le regole che hanno confidenza più alta, sto prendendo le regole che mi forniscono più informazioni. Un'altra misura che ci interessa mantenere alta nel caso dei frequent itemset è l'\textit{interesse}:
\begin{center}
    \begin{math}
        Interest(I \rightarrow j) = |conf(I \rightarrow j) - Pr(j)|
    \end{math}
\end{center}

\subsection{Representation of the Market-Basket data}
Tipicamente, i market basket data sono salvati in un file, basket-by-basket: \{ 22, 33, 44 \} ad esempio potrebbe essere la rappresentazione di un basket. 
\\
Il numero massimo di elementi che abbiamo sono le coppie, perchè abbiamo detto che per essere frequenti un insieme di elementi deve avere delle coppie frequenti. Gli algoritmi che vedremo ora funzionano solo su coppie o terne frequenti perchè non ha senso ragionare su valori non frequenti. 
\\
\textbf{Domanda da esame: se io avessi un dataset gigantesco dove ogni riga è uno scontrino, come conto le coppie frequenti?} La cosa più banale che ci viene in mente sarebbe chiedere una lista di tutti gli elementi e con un contatore, fare il conteggio. Poi con un ciclo for per le possibili coppie, terne, ecc... Ma in un mondo big data, questo approccio è impossibile. Bisogna sfruttare il concetto di monotonicità introdotto in precedenza, e lo faccio leggendo il dataset e cercando \textbf{solo} gli elementi frequenti (diminuendo il conteggio) per poi creare delle coppie da essi e vedere quelle frequenti, e così via. Con questo approccio, ad ogni passaggio, diminuisco il numero totale di elementi da leggere. 
\\
Se avessi continuato secondo la linea d'azione originale, avrei dovuto creare (con il contatore) per ogni scontrino, un intero per vedere quante volte si ripete con tutti gli altri: per ogni scontrino avrei creato dei vettori di contatori grandi GB.
\\
Tutti gli approcci che andremo a vedere, sono caratterizzati dal numero di volte che leggo i dati.

\subsubsection{Triangular Matrix e Triplets}
Sono due tecniche che utilizziamo per fare lo storage in memoria del valore del contatore (altra operazione che ha un costo computazionale smisurato):
\begin{itemize}
    \item \textbf{Triangular Matrix:} posso considerare il vettore dei valori associando ad ogni posizione di quel valore uno specifico riferimento ad una coppia. Sui singoletti è facile, perchè l'elemento 0 è il primo elemento e quello in posto 1000 è il 1001esimo. Per le coppie è più complesso. 
    \\ 
    Come faccio con le coppie? Mi serve una funzione che mi associ ad ogni posizione, una coppia di riferimento. 
    \begin{center}
        \begin{math}
            k = (i-1)(n-\frac{i}{2} + j - i)
        \end{math}
    \end{center}
    Dove k è la posizione nel vettore, n è il numero massimo di prodotti, i e j è la coppia che voglio trovare. 
    \\
    \textit{Esempio.} Immaginiamo di avere 5 elementi, da 1 a 5. Dove sarà la coppia \{1, 4\}? 
    \begin{center}
        \begin{math}
            k = (1-1)(5-\frac{1}{2} + 4 - 1)
        \end{math}
    \end{center}
    k = 3. La coppia scelta è il terzo elemento del vettore. \textbf{Importante!} Dobbiamo considerare che i sia minore di j così da evitare i duplicati.
    \item \textbf{Triplets:} la tecnica delle triplette consiste nella creazione di una tripletta composta da i due valori numerici e il conteggio: \{1, 2, 3\} ma è svantaggioso rispetto alla matrice poichè per una coppia salva 3 interi invece di uno solo. Ha però un vantaggio enorme sulla matrice: tratta le coppie non esistenti. Nella matrice, a prescindere che una coppia si verifichi o meno, ha un posto all'interno del vettore quindi occupo la memoria con uno 0. Nel caso invece delle triplette, le coppie con conteggio nullo vengono scartate! Ed è quindi molto utile quando si usano dataset con poche coppie frequenti, altrimenti si preferisce la triangular matrix.
\end{itemize} 

\subsubsection{A-Priori Algorithm}
Creato per ridurre il numero di coppie da dover contare, fa due passaggi sui dati:
\begin{enumerate}
    \item creiamo 2 tabelle, dove la prima traduce i nomi degli items in interi. Mentre l'altra tabella è un array di conteggi. Fondamentalmente il primo step è quello utilizzato per ricercare gli elementi frequenti, candidandoli
    \item cerchiamo i singoletti non frequent e li eliminiamo. Quali sono le coppie candidate? Tutte le coppie degli elementi rimamenti, per il principio di monotonicità! 
\end{enumerate}
E alla fine valuta quali sono le coppie frequenti dopo il secondo passaggio (e non è detto che siano tutte frequent). Sicuramente efficace, ma è l'algoritmo più lento. 
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/apriori.png}
\end{figure}

\subsubsection{Park, Chen, Yu Algorithm}
Simile al algoritmo precedente, sfrutta la possibilità di candidare direttamente delle coppie. Quindi nel primo passaggio avviene la generazione di coppie candidate. Utilizzando una funzione di hash non facciamo lo storage delle coppie (sarebbe troppo costoso) ma associamo ad ogni coppia un valore numerico. Quali saranno le coppie frequenti? Quelle formate da singoletti frequenti e quelle in bucket frequenti. 
\\
\textit{Esempio.} Immaginiamo di avere questa serie di elementi:
\begin{center}
    \{1,2,3\}, \{1,3,4\}, \{1,2\},  \{1,2,4\}, \{1,2,3,4\}, \{1,4\}
\end{center}
Creiamo n funzioni di hash, che chiameremo h, che contano le coppie in questo modo: h1, conta le coppie con 1, h2 quelle con 2, ecc... Il risultato è che grazie alle coppie generate dalle funzioni di hash possiamo escludere le coppie non frequenti. 

\newpage

\subsubsection{Limited Pass Algorithm}
Tecnica approssimata che non trova tutti gli item frequenti. Ragiona in un modo completamente diverso dagli altri algoritmi perchè estrae una parte degli items che possano stare in memoria e li estrae a sorte. Modifico la soglia di supporto, poichè con un dataset più piccolo, non posso usare ancora l'1\% del dataset ma prendere l'1\% del dataset più piccolo. Una volta ridotto il set, calcolo con un qualunque algoritmo visto finora. Il risultato che ottengo, impongo che sia il risultato del dataset completo, ma ovviamente lavorare solamente su un subset produrrà molti errori. Soprattutto, il problema principale saranno i falsi positivi: come posso ridurli? Faccio una seconda passata sul dataset leggendo gli elementi e verifico se quelli da me trovati sono frequenti! Elimino i non-frequent. I falsi negativi non sai di averli, e purtroppo è inutile cercare di correggerli, perchè vorrebbe dire applicare uno degli altri algoritmi...Posso pensare di ridurre i falsi negativi campionando un'altra parte del dataset e ripetendo le operazioni su di essa. Se dovessi trovare altri valori frequent che non lo erano nel primo subset, basterebbe aggiornarli. 
\\
C'è un'altra operazione, un po' meno dispendiosa: abbasso la soglia.  

\subsubsection{Toivonen's Algorithm}
Trova i risultati in queste due condizioni:
\begin{itemize}
    \item Trova tutti i risultati, senza falsi positivi o falsi negativi
    \item Non genera alcuna risposta
\end{itemize}
Come funziona? Sfrutta il concetto di \textit{negative border.} Gli itemset non frequenti formati da elementi frequenti (più corretto sarebbe dire itemset non frequenti, i cui sottoinsiemi sono frequenti). 
\\
Prendiamo solo una porzione, aggiorniamo la soglia, e calcoliamo gli elementi frequenti come visto nel Limited Pass. Andiamo a costruire il negative border e lo confrontiamo con il dataset nella sua interezza: se ci sono insiemi del negative border che sono frequent nel dataset, allora non produce risultato. Altrimenti il risultato ottenuto è corretto. 

\newpage

\subsubsection{SON Algorithm (Savasere, Omiecinski and Navathe)}
Dividiamo l'input in chunks e faccio andare l'algoritmo su ogni chunk. Ovviamente è necessario aggiornare la soglia! 
\\
Abbiamo diviso il carico computazionale tra n macchine. Ogni macchina mi darà un insieme di elementi frequent: paragoniamo il risultato con quello ottenuto dalle altre macchine. Possiamo dire che un elemento è frequent solo se supera la soglia anche nelle altre macchine (quindi se è frequent nelle altre). 
\\
\textit{Esempio.} Immaginiamo di avere due chunks:
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/SONalgorithm.png}
\end{figure}
\\
Con le relative soglie. M1 mi estrae 4 elementi frequenti F1, F4, F7 e F9 mentre M2 mi estrae FA, F1, F4. Ora prendo l'insieme degli elementi e faccio controllare se sono frequenti in entrambe le macchine. Praticamente, passo ad M1 e M2 l'insieme di valori complessivo e conto le occorrenze in uno e nell'altro caso, se superano la soglia li posso tenere.
\\
\textbf{Approccio con MapReduce.} Il primo map mi analizza un pezzo e mi trova quelli frequenti. Mi emette una coppia chiave valore con il valore frequente e una chiave associata: per esempio. potrebbe venire una cosa di questo tipo (F1, 1), (F2, 1). Il reducer non fa assolutamente niente, butta fuori le coppie esattamente in questo modo per fare prima. Conviene piuttosto fare due cicli piuttosto che fare tutto in una volta. 
\\
Secondo step: altro mapreduce, mettiamo M1 e M2 ma questa volta con gli elementi frequenti: con il combiner possiamo contare quante occorrenze abbiamo in ogni macchina. 

\newpage

\subsection{Finding Similar Items}
Abbiamo visto come trovare gli items più frequenti, ora dobbiamo cercare di capire come trovare gli items simili. Dobbiamo trovare una misura per stabilire una similarità tra due elementi: \textbf{similarità di Jaccard}. Ossia, va analizzato un oggetto secondo un insieme di proprietà e poi è necessario guardare le proprietà che sono in comune rispetto a tutte le proprietà con cui qualifichiamo gli oggetti. Per una persona, potrebbero essere altezza, colore degli occhi, voce... Su un documento ad esempio, le parole uguali. 
\\
Perchè è utile? Per trovare documenti uguali, quindi plagiati, oppure per trovare i suggerimenti di acquisto per le persone; si vede bene la pipeline guardando la seguente immagine:
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/pipeline.png}
\end{figure}
\\
Tutti i passaggi verranno spiegati nello specifico qui di seguito. 

\subsubsection{Shingling}
Prima tecnica: convertiamo i documenti in insiemi. Indivuiamo una finestra di k caratteri chiamati \textit{shingle.} Vado a costruire una matrice dove sulle colonne ho sempre un milione di documenti mentre sulle righe tutte le possibili combinazioni tra caratteri. 
\\
\textit{Esempio.} Cerchiamo di capire meglio il concetto di shingle da un esempio. Frase: Oggi piove
\\
Se prendiamo k uguale a 3, otteniamo come shingle ogg, ggi, gi\_ ecc... 
\\
\textbf{Ma perchè utilizzare degli shingle?} Mi dà un ordine alle parole con questa finestra che scorre. E come scelgo k? Con un k basso, tutti i documenti risultano uguali, non faccio abbastanza distinzione se guardo in finestre unitarie di singole lettere; se k è troppo alto, ottengo al contrario una ricerca troppo pignola dove trovo solo documenti esattamente uguali (considero interi discorsi in una finestra). 
\\
\textbf{Quante righe posso fare con k=3?} Si calcolano come i caratteri che scelgo, elevati alla k: $(caratteri)^k$ e devo fare attenzione perchè per k troppo elevati, non staranno mai in memoria. 

\newpage

Se voglio confrontare due documenti, andrò a scrivere: 
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/shingling.png}
\end{figure}
\\
Dove tutti quei 0 e 1 rappresentano gli shingle per ogni documento, calcolerò la similarità di Jaccard facendo numero elementi in comune diviso numero elementi totali in esame. Stabilisco una soglia, se la superano sono simili. Ma come posso fare se devo confrontare molti documenti? 
\subsubsection{Minhashing}
Seconda tecnica: tecnica per mantenere la similarità con una tabella ridotta. Riduce il numero di righe mettendo per ogni vettore della tabella originale una signature o firma che mantiene la similarità. 
\\
Un primo approccio, tornando all'esempio precedente della tabella in alto: con valori booleani, posso trattarli da tali, quindi calcolarne il valore in decimale per ogni riga (ad esempio, 101 sarebbe 5) e riduco tanto il numero di righe (attenzione! la compattazione avviene lungo la colonna, non lungo le righe quindi in verticale!).
\\
Ma c'è un problema. Compattare tra di loro degli shingle, potrebbe crearmi combinazioni di lettere improbabili e trovare due documenti con tale sequenza simile diventa impossibile.
\\
\textbf{Altra soluzione.} Torniamo ad utilizzare la funzione di hash. Numeriamo le righe: voglio convertire quella matrice in una matrice più corta. Facciamo un esempio con una matrice da un milione di righe, voglio trasformarla in una matrice da mille righe. Dopo aver dato un indice ad ogni riga, lo divido per mille e calcolo il resto; se in quel punto della matrice poi c'è un uno, nella matrice ridotta ci sarà un uno. Guardando sempre l'immagine in alto: l'elemento 1 diviso mille fa 0 con resto di 1. Quindi nella nuova matrice andrò a mettermi nella posizione 1, e ora devo guardare se è uno 0 o un 1 nella prima matrice: è 1. Quindi nella posizione 1 della nuova matrice andrò a mettere 1. 
\\
La stessa identica cosa accadrà per il valore 1001, ad esempio, finirà anche lui nella posizione 1. Quindi sto mappando all'interno della stessa cella, più shingle; ciò genera errore. Vuole dire che se viene mappato all'interno della cella uno shingle frequent come può essere 'to' in lingua italiana, insieme allo shingle 'zy' stiamo dando valore 1 anche ad uno shingle non-frequent, ma non è un problema dato che stiamo accorciando il numero di righe.

\newpage

\textbf{Minhashing.} Sfruttiamo quindi il concetto di hashing per trovare una matrice ridotta, che mantenga la similarità di Jaccard. Creiamo le signature: 
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/minhashing.png}
\end{figure}
\\
Costruiamo una permutazione delle righe, le righe vengono cioè rimescolate (nell'immagine, l'ordine permutato è dato dalla colonna rossa, gialla e azzurra). Fatta la permutazione, vado a costruire una matrice di signature in cui il numero che ho è la prima riga in cui compare un 1 in tale permutazione. Ad esempio, nell'immagine, la prima cella della rossa è 2, perchè compare un 1 nella sua riga 2 (che è la riga 1 della matrice originale). Ovviamente le colonne della matrice di signature corrispondono ai documenti, come nella matrice iniziale. Quindi quello che voglio è che la similarità di Jaccard calcolata per i due documenti sia uguale alla probabilità che le signature nella matrice ridotta siano simili:
\begin{center}
    \begin{math}
        SIM_j(D1,D2) = P(S(D1) = S(D2))  
    \end{math}
\end{center}
Dove \textit{S} è la signature. Mi aspetterò che due documenti simili, abbiano signature simili. 
\\
\textit{Dimostrazione.} Vogliamo dimostrare la similarità tra due documenti aventi simili signature.
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/c1c2.png}
\end{figure}
\\
Distinguo tra righe X (entrambi 1) righe Y (diversi) e righe Z (entrambi 0). Sappiamo che un calcolo possibile è ridurre il numero di righe eliminando quelle di tipo Z inutili nel calcolo della similarità. Al numeratore della similarità di Jaccard avremo X dove sono simili e al denominatore le righe X + Y. 
\\
Consideriamo una permutazione, e non calcoliamo le righe Z. Qual è la probabilità togliendo gli Z di avere una X prima di una riga Y? (ricordati come viene costruita la matrice di signature) Essa è: 
\begin{center}
    $\frac{X}{X+Y}$ 
\end{center}
Ma se abbiamo una X prima delle altre, cosa succede nel minhashing? Che i due documenti avevano lo stesso valore. Quindi la probabilità che i documenti avessero lo stesso numero, è pari alla similarità tra i due documenti.

\subsubsection{Minhashing optimization}
A lezione sono stati spiegati un paio di metodi per automatizzare e migliorare il processo di Minhashing, qui di seguito ne vediamo uno.
\\
Dopo la permutazione, noi abbiamo un numero di righe ridotto, con un valore di similarità uguale. Non è necessario permutare tutte le righe, posso anche semplicemente fare una permutazione di k righe dove k è minore del numero di righe n. L'unico caso in cui questa operazione può risultare deleteria, sarebbe nel caso in cui alcuni valori non dovessero trovare una corrispondenza; ma per la costrizione della matrice di hashing, in corrispondenza di quei valori troveremmo allora un infinito, che è comunque diverso da qualunque altro valore nella matrice:
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/matrixa.png}
\end{figure}
\\
Se considerassi k = 3, quel 4 non ci sarebbe, e rimarrebbe comunque un elemento che non porta informazione essendo diverso da tutti gli altri. Potrebbe valere 4 o 1000 che non mi interesserebbe. C'è però un problema. Se scelgo un k troppo basso (ad esempio nella matrice qua sopra un valore critico potrebbe essere k=2) la matrice potrebbe perdere troppe informazioni e con troppi valori ad infinito non posso lavorarci perchè potrebbe capitare che se due documenti erano diversi, considerando k molto basso potrebbero perdere molti elementi diversi che verrebbero sostituiti da degli infiniti diventando uguali e trasformando documenti non simili in documenti simili. 

\subsubsection{Locality-sensitive hashing}
Terzo step: ora che ho ridotto le righe, voglio poter fare il numero più ridotto possibile di confronti tra le colonne. Con LSH lavoriamo sulla matrice di minhashing, per poi andare ad utilizzare una funzione che candidi delle coppie. Creo dei gruppi di documenti che possibilmente sono coppie di elementi simili. Come li trovo? Ad esempio potrei calcolare la somma nella matrice di minhashing  dei valori di ogni documento, e quelli con valori di somma simile sono documenti simili. Quale può essere il problema di questa funzione' Che basta che cambi uno dei valori e categorizza diverso (ad esempio documento 2,2,1 come nell'immagine e 2,4,1). Quindi posso avere dei falsi negativi. Mentre i falsi positivi non sono un grosso problema:  i valori di somma possono venire uguali, ma poi vanno osservati sul documento. LSH compensa quindi il problema dei falsi negativi suddividendo in bande la matrice e applicando la funzione di hash ad ogni banda. Basta infatti che anche solo una banda abbia gli stessi valori che vado a finire nello stesso bucket. 

\newpage

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/lsh.png}
\end{figure}
Questo procedimento diminuisce il costo dei confronti significativamente tanto. 
\\
\textbf{Importante.} Due colonne vanno nello stesso bucket quando hanno gli stessi valori ovvero quando i due documenti hanno un'alta similarità di Jaccard. La probabilità che le signatures abbiano lo stesso valore in una banda (e diventino quindi coppie candidate) è:
\begin{center}
    \begin{math}
        1 - (1 - s^r)^b
    \end{math}
\end{center}
Dove s è il valore di similarità, r il numero di righe e b il numero di bande. Quindi possiamo manipolare questo valore: 
\\
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{FrequentItems/img/graph.png}
\end{figure}
\\
Se uso b e r uguali a 1, la probabilità di essere nello stesso bucket è uguale alla similarità, il che mi va bene ma per ridurre il numero di falsi negativi, devo cambiare funzione. 
\\
\textit{Esempi.} Troviamo coppie con s = 0.8, mettiamo b=20 e r=5. Facendo con dei documenti C1 e C2 la cui sim(C1,C2) = 0.3 vedremo:
\begin{itemize}
    \item probabilità che C1 e C2 siano identici in una banda = $0.3^5 = 0.00243$
    \item probabilità che C1 e C2 siano uguali in almeno 1 delle 20 bande = $1-(1-0.00243)^{20} = 0.0474$
\end{itemize}
Di conseguenza, la probabilità di avere delle coppie candidate con 0.3 similarità, sarà intorno al 4\%, valore accettabile. Il caso opposto sarebbe pensare alle coppie con 0.8 similarità: 
\begin{itemize}
    \item probabilità che C1 e C2 siano identici in una banda = $0.8^5 = 0.328$
    \item probabilità che C1 e C2 siano uguali in almeno 1 delle 20 bande = $1-(1-0.328)^{20} = 0.00035$ 
\end{itemize}
Questo significa che circa 1 coppia su 3000 che esaminiamo (con 0.8 o più di similarità) viene considerata falso negativo. Anche questo errore è più che accettabile (troviamo il 99.965\% di coppie nei documenti simili).